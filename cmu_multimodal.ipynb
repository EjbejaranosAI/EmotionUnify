{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU Mutlimodalsdk - Exploration dataset and Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE       __init_.py    librerias.txt next_steps.md \u001b[34mwandb\u001b[m\u001b[m\n",
      "LICENSE.txt   \u001b[31mclean.sh\u001b[m\u001b[m      \u001b[34mmmsdk\u001b[m\u001b[m         optim.std\n",
      "README.md     \u001b[34mexamples\u001b[m\u001b[m      model.std     \u001b[34mrelated_repos\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "# path to the SDK folder\n",
    "SDK_PATH: Optional[str] = None\n",
    "# path to the folder where you want to store data\n",
    "DATA_PATH: Optional[str] = '.src/datasets/CMU-MultimodalSDK/data/'\n",
    "# path to a pretrained word embedding file\n",
    "WORD_EMB_PATH: Optional[str] = None\n",
    "# path to loaded word embedding matrix and corresponding word2id mapping\n",
    "CACHE_PATH: Optional[str] = '.src/datasets/CMU-MultimodalSDK/data/embedding_and_mapping.pt'\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder where you want to store data\n",
    "os.chdir('./src/datasets/CMU-MultimodalSDK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE       __init_.py    librerias.txt next_steps.md \u001b[34mwandb\u001b[m\u001b[m\n",
      "LICENSE.txt   \u001b[31mclean.sh\u001b[m\u001b[m      \u001b[34mmmsdk\u001b[m\u001b[m         optim.std\n",
      "README.md     \u001b[34mexamples\u001b[m\u001b[m      model.std     \u001b[34mrelated_repos\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "import sys\n",
    "import mmsdk\n",
    "from mmsdk import mmdatasdk as md\n",
    "from subprocess import check_call, CalledProcessError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE       README.md     \u001b[31mclean.sh\u001b[m\u001b[m      \u001b[34mmmsdk\u001b[m\u001b[m         \u001b[34mrelated_repos\u001b[m\u001b[m\n",
      "LICENSE.txt   __init_.py    \u001b[34mexamples\u001b[m\u001b[m      next_steps.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK path is not specified! Please specify first in constants/paths.py\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl Kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. Revise el código de las celdas para identificar una posible causa del error. Haga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. Vea el [registro] de Jupyter (command:jupyter.viewOutput) para obtener más detalles."
     ]
    }
   ],
   "source": [
    "if SDK_PATH is None:\n",
    "    print(\"SDK path is not specified! Please specify first in constants/paths.py\")\n",
    "    exit(0)\n",
    "else:\n",
    "    sys.path.append(SDK_PATH)\n",
    "\n",
    "# create folders for storing the data\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    check_call(' '.join(['mkdir', '-p', DATA_PATH]), shell=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data (Only neccesary to do it one time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1m[2023-09-22 09:51:16.977] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/language/CMU_MOSI_TimestampedWordVectors.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_TimestampedWordVectors.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 09:52:32.837] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 09:52:32.846] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_TimestampedWordVectors.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:52:32.863] | Status  | \u001b[0mChecking the integrity of the <glove_vectors> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:52:32.863] | Status  | \u001b[0mChecking the format of the data in <glove_vectors> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 09:52:32.891] | Success | \u001b[0m<glove_vectors> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:52:32.891] | Status  | \u001b[0mChecking the format of the metadata in <glove_vectors> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 09:52:32.891] | Warning | \u001b[0m<glove_vectors> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[94m\u001b[1m[2023-09-22 09:52:33.138] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/visual/CMU_MOSI_Visual_Facet_41.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Visual_Facet_41.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 09:53:56.671] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 09:53:56.675] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Visual_Facet_41.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:53:56.692] | Status  | \u001b[0mChecking the integrity of the <FACET_4.1> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:53:56.692] | Status  | \u001b[0mChecking the format of the data in <FACET_4.1> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 09:53:56.726] | Success | \u001b[0m<FACET_4.1> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:53:56.726] | Status  | \u001b[0mChecking the format of the metadata in <FACET_4.1> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 09:53:56.726] | Warning | \u001b[0m<FACET_4.1> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[94m\u001b[1m[2023-09-22 09:53:56.987] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/visual/CMU_MOSI_Visual_Facet_42.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Visual_Facet_42.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 09:55:02.927] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 09:55:02.929] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Visual_Facet_42.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:55:02.941] | Status  | \u001b[0mChecking the integrity of the <FACET_4.2> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:55:02.941] | Status  | \u001b[0mChecking the format of the data in <FACET_4.2> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 09:55:02.974] | Success | \u001b[0m<FACET_4.2> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:55:02.974] | Status  | \u001b[0mChecking the format of the metadata in <FACET_4.2> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 09:55:02.974] | Warning | \u001b[0m<FACET_4.2> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[94m\u001b[1m[2023-09-22 09:55:03.239] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/acoustic/CMU_MOSI_OpenSmile_EB10.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_OpenSmile_EB10.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 09:55:19.269] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 09:55:19.270] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_OpenSmile_EB10.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:55:19.274] | Status  | \u001b[0mChecking the integrity of the <OpenSmile_emobase2010> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:55:19.274] | Status  | \u001b[0mChecking the format of the data in <OpenSmile_emobase2010> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 09:55:19.295] | Success | \u001b[0m<OpenSmile_emobase2010> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 09:55:19.295] | Status  | \u001b[0mChecking the format of the metadata in <OpenSmile_emobase2010> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 09:55:19.295] | Warning | \u001b[0m<OpenSmile_emobase2010> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[94m\u001b[1m[2023-09-22 09:55:19.551] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/acoustic/CMU_MOSI_openSMILE_IS09.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_openSMILE_IS09.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:05:27.038] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 10:05:27.040] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_openSMILE_IS09.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:05:27.052] | Status  | \u001b[0mChecking the integrity of the <b'OpenSMILE'> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:05:27.052] | Status  | \u001b[0mChecking the format of the data in <b'OpenSMILE'> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:05:27.072] | Success | \u001b[0m<b'OpenSMILE'> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:05:27.072] | Status  | \u001b[0mChecking the format of the metadata in <b'OpenSMILE'> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 10:05:27.072] | Warning | \u001b[0m<b'OpenSMILE'> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[94m\u001b[1m[2023-09-22 10:05:27.340] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/visual/CMU_MOSI_Visual_OpenFace_1.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Visual_OpenFace_1.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:17:06.541] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 10:17:06.545] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Visual_OpenFace_1.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:17:06.561] | Status  | \u001b[0mChecking the integrity of the <OpenFace_1> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:17:06.561] | Status  | \u001b[0mChecking the format of the data in <OpenFace_1> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:17:06.596] | Success | \u001b[0m<OpenFace_1> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:17:06.596] | Status  | \u001b[0mChecking the format of the metadata in <OpenFace_1> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 10:17:06.596] | Warning | \u001b[0m<OpenFace_1> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[94m\u001b[1m[2023-09-22 10:17:06.977] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/visual/CMU_MOSI_Visual_OpenFace_2.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Visual_OpenFace_2.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:29:44.959] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 10:29:44.974] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Visual_OpenFace_2.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:29:44.991] | Status  | \u001b[0mChecking the integrity of the <OpenFace_2> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:29:44.991] | Status  | \u001b[0mChecking the format of the data in <OpenFace_2> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:29:45.011] | Success | \u001b[0m<OpenFace_2> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:29:45.011] | Status  | \u001b[0mChecking the format of the metadata in <OpenFace_2> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 10:29:45.011] | Warning | \u001b[0m<OpenFace_2> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[94m\u001b[1m[2023-09-22 10:29:45.266] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/acoustic/CMU_MOSI_COVAREP.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_COVAREP.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:36:35.141] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 10:36:35.145] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_COVAREP.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:35.159] | Status  | \u001b[0mChecking the integrity of the <COVAREP> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:35.159] | Status  | \u001b[0mChecking the format of the data in <COVAREP> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:36:35.194] | Success | \u001b[0m<COVAREP> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:35.194] | Status  | \u001b[0mChecking the format of the metadata in <COVAREP> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 10:36:35.194] | Warning | \u001b[0m<COVAREP> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-22 10:36:35.194] | Success | \u001b[0mDataset initialized successfully ... \n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:35.457] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/language/CMU_MOSI_TimestampedWords.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_TimestampedWords.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:36:44.206] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 10:36:44.207] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_TimestampedWords.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:44.212] | Status  | \u001b[0mChecking the integrity of the <words> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:44.212] | Status  | \u001b[0mChecking the format of the data in <words> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:36:44.241] | Success | \u001b[0m<words> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:44.241] | Status  | \u001b[0mChecking the format of the metadata in <words> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 10:36:44.241] | Warning | \u001b[0m<words> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:44.530] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/language/CMU_MOSI_TimestampedPhones.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_TimestampedPhones.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:36:47.505] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 10:36:47.506] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_TimestampedPhones.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:47.513] | Status  | \u001b[0mChecking the integrity of the <phoneme> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:47.513] | Status  | \u001b[0mChecking the format of the data in <phoneme> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:36:47.542] | Success | \u001b[0m<phoneme> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:47.542] | Status  | \u001b[0mChecking the format of the metadata in <phoneme> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 10:36:47.542] | Warning | \u001b[0m<phoneme> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-22 10:36:47.542] | Success | \u001b[0mDataset initialized successfully ... \n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:47.802] | Status  | \u001b[0mDownloading from http://immortal.multicomp.cs.cmu.edu/CMU-MOSI/labels/CMU_MOSI_Opinion_Labels.csd to .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Opinion_Labels.csd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:36:48.798] | Success | \u001b[0mDownload complete!\n",
      "\u001b[92m\u001b[1m[2023-09-22 10:36:48.799] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Opinion_Labels.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:48.805] | Status  | \u001b[0mChecking the integrity of the <Opinion Segment Labels> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:48.805] | Status  | \u001b[0mChecking the format of the data in <Opinion Segment Labels> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-22 10:36:48.833] | Success | \u001b[0m<Opinion Segment Labels> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-22 10:36:48.833] | Status  | \u001b[0mChecking the format of the metadata in <Opinion Segment Labels> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-22 10:36:48.833] | Warning | \u001b[0m<Opinion Segment Labels> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-22 10:36:48.833] | Success | \u001b[0mDataset initialized successfully ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# MOSI DATASET\n",
    "DATASET = md.cmu_mosi\n",
    "\n",
    "try:\n",
    "    md.mmdataset(DATASET.highlevel, DATA_PATH)\n",
    "except RuntimeError:\n",
    "    print(\"High-level features have been downloaded previously.\")\n",
    "\n",
    "try:\n",
    "    md.mmdataset(DATASET.raw, DATA_PATH)\n",
    "except RuntimeError:\n",
    "    print(\"Raw data have been downloaded previously.\")\n",
    "\n",
    "try:\n",
    "    md.mmdataset(DATASET.labels, DATA_PATH)\n",
    "except RuntimeError:\n",
    "    print(\"Labels have been downloaded previously.\")\n",
    "\n",
    "!wget https://github.com/Justin1904/CMU-MultimodalSDK-Tutorials/blob/master/data/CMU_MOSI_ModifiedTimestampedWords.csd -O .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_ModifiedTimestampedWords.csd\n",
    "data_files = os.listdir(DATA_PATH)\n",
    "\n",
    "print(\"Downloaded data: \",'\\n'.join(data_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data:  CMU_MOSI_ModifiedTimestampedWords.csd\n",
      "CMU_MOSI_OpenSmile_EB10.csd\n",
      "CMU_MOSI_openSMILE_IS09.csd\n",
      "CMU_MOSI_Opinion_Labels.csd\n",
      "CMU_MOSI_TimestampedWords.csd\n",
      "CMU_MOSI_Visual_OpenFace_2.csd\n",
      "CMU_MOSI_TimestampedWordVectors.csd\n",
      "CMU_MOSI_Visual_OpenFace_1.csd\n",
      "CMU_MOSI_Visual_Facet_41.csd\n",
      "CMU_MOSI_TimestampedPhones.csd\n",
      "CMU_MOSI_Visual_Facet_42.csd\n",
      "CMU_MOSI_COVAREP.csd\n"
     ]
    }
   ],
   "source": [
    "data_files = os.listdir(DATA_PATH)\n",
    "\n",
    "print(\"Downloaded data: \",'\\n'.join(data_files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligned modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:58:56.575] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_ModifiedTimestampedWords.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:58:56.587] | Status  | \u001b[0mChecking the integrity of the <b'CMU_MOSI_ModifiedTimestampedWords'> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:58:56.587] | Status  | \u001b[0mChecking the format of the data in <b'CMU_MOSI_ModifiedTimestampedWords'> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:58:56.621] | Success | \u001b[0m<b'CMU_MOSI_ModifiedTimestampedWords'> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:58:56.621] | Status  | \u001b[0mChecking the format of the metadata in <b'CMU_MOSI_ModifiedTimestampedWords'> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:58:56.621] | Warning | \u001b[0m<b'CMU_MOSI_ModifiedTimestampedWords'> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-24 17:58:56.622] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Visual_Facet_41.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:58:56.634] | Status  | \u001b[0mChecking the integrity of the <FACET_4.1> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:58:56.634] | Status  | \u001b[0mChecking the format of the data in <FACET_4.1> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:58:56.662] | Success | \u001b[0m<FACET_4.1> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:58:56.662] | Status  | \u001b[0mChecking the format of the metadata in <FACET_4.1> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:58:56.662] | Warning | \u001b[0m<FACET_4.1> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-24 17:58:56.663] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_COVAREP.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:58:56.672] | Status  | \u001b[0mChecking the integrity of the <COVAREP> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:58:56.672] | Status  | \u001b[0mChecking the format of the data in <COVAREP> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:58:56.695] | Success | \u001b[0m<COVAREP> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:58:56.695] | Status  | \u001b[0mChecking the format of the metadata in <COVAREP> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:58:56.695] | Warning | \u001b[0m<COVAREP> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-24 17:58:56.695] | Success | \u001b[0mDataset initialized successfully ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "visual_field = 'CMU_MOSI_Visual_Facet_41'\n",
    "acoustic_field = 'CMU_MOSI_COVAREP'\n",
    "#text_field = 'CMU_MOSI_TimestampedWords'\n",
    "text_field = 'CMU_MOSI_ModifiedTimestampedWords'\n",
    "\n",
    "\n",
    "features = [\n",
    "    text_field,\n",
    "    visual_field,\n",
    "    acoustic_field\n",
    "]\n",
    "\n",
    "recipe = {feat: os.path.join(DATA_PATH, feat) + '.csd' for feat in features}\n",
    "dataset = md.mmdataset(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Dataset keys: ['CMU_MOSI_ModifiedTimestampedWords', 'CMU_MOSI_Visual_Facet_41', 'CMU_MOSI_COVAREP']\n",
      "================================================================================\n",
      "IDs: ['03bSnISJMiM', '0h-zjBukYpk', '1DmNV9C1hbY', '1iG0909rllw', '2WGyTLYerpo']\n",
      "================================================================================\n",
      "Into IDs: ['features', 'intervals']\n",
      "Shape: [5404, 2]\n",
      "================================================================================\n",
      "Shape visual features: [5404, 47]\n",
      "Shape text features: [658, 1]\n",
      "Shape acoustic features: [18009, 74]\n",
      "Different modalities have different number of time steps!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"Dataset keys: {list(dataset.keys())}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"IDs: {list(dataset[visual_field].keys())[:5]}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "some_id = list(dataset[visual_field].keys())[15]\n",
    "print(f\"Into IDs: {list(dataset[visual_field][some_id].keys())}\")\n",
    "print(f\"Shape: {list(dataset[visual_field][some_id]['intervals'].shape)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Shape visual features: {list(dataset[visual_field][some_id]['features'].shape)}\")\n",
    "print(f\"Shape text features: {list(dataset[text_field][some_id]['features'].shape)}\")\n",
    "print(f\"Shape acoustic features: {list(dataset[acoustic_field][some_id]['features'].shape)}\")\n",
    "print(\"Different modalities have different number of time steps!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1m[2023-09-24 17:59:03.319] | Status  | \u001b[0mUnify was called ...\n",
      "\u001b[92m\u001b[1m[2023-09-24 17:59:03.319] | Success | \u001b[0mUnify completed ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:03.319] | Status  | \u001b[0mPre-alignment based on <CMU_MOSI_ModifiedTimestampedWords> computational sequence started ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:07.908] | Status  | \u001b[0mPre-alignment done for <CMU_MOSI_COVAREP> ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:08.852] | Status  | \u001b[0mPre-alignment done for <CMU_MOSI_Visual_Facet_41> ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:08.872] | Status  | \u001b[0mAlignment starting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.413] | Success | \u001b[0mAlignment to <CMU_MOSI_ModifiedTimestampedWords> complete.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.413] | Status  | \u001b[0mReplacing dataset content with aligned computational sequences\n",
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.425] | Success | \u001b[0mInitialized empty <CMU_MOSI_ModifiedTimestampedWords> computational sequence.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.425] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.476] | Success | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.476] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:59:34.476] | Warning | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.476] | Success | \u001b[0mInitialized empty <CMU_MOSI_Visual_Facet_41> computational sequence.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.476] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_Visual_Facet_41> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.517] | Success | \u001b[0m<CMU_MOSI_Visual_Facet_41> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.517] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_Visual_Facet_41> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:59:34.518] | Warning | \u001b[0m<CMU_MOSI_Visual_Facet_41> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.518] | Success | \u001b[0mInitialized empty <CMU_MOSI_COVAREP> computational sequence.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.518] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_COVAREP> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.556] | Success | \u001b[0m<CMU_MOSI_COVAREP> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.556] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_COVAREP> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:59:34.556] | Warning | \u001b[0m<CMU_MOSI_COVAREP> computational sequence does not have all the required metadata ... continuing \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def avg(intervals: np.array, features: np.array) -> np.array:\n",
    "    try:\n",
    "        return np.average(features, axis=0)\n",
    "    except:\n",
    "        return features\n",
    "dataset.align(text_field, collapse_functions=[avg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.599] | Success | \u001b[0mComputational sequence read from file .src/datasets/CMU-MultimodalSDK/data/CMU_MOSI_Opinion_Labels.csd ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.605] | Status  | \u001b[0mChecking the integrity of the <Opinion Segment Labels> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.605] | Status  | \u001b[0mChecking the format of the data in <Opinion Segment Labels> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.619] | Success | \u001b[0m<Opinion Segment Labels> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.619] | Status  | \u001b[0mChecking the format of the metadata in <Opinion Segment Labels> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:59:34.619] | Warning | \u001b[0m<Opinion Segment Labels> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.619] | Status  | \u001b[0mUnify was called ...\n",
      "\u001b[92m\u001b[1m[2023-09-24 17:59:34.651] | Success | \u001b[0mUnify completed ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.653] | Status  | \u001b[0mPre-alignment based on <CMU_MOSI_Opinion_Labels> computational sequence started ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.712] | Status  | \u001b[0mPre-alignment done for <CMU_MOSI_COVAREP> ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.770] | Status  | \u001b[0mPre-alignment done for <CMU_MOSI_ModifiedTimestampedWords> ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.830] | Status  | \u001b[0mPre-alignment done for <CMU_MOSI_Visual_Facet_41> ...\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:34.832] | Status  | \u001b[0mAlignment starting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:35.897] | Success | \u001b[0mAlignment to <CMU_MOSI_Opinion_Labels> complete.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:35.897] | Status  | \u001b[0mReplacing dataset content with aligned computational sequences\n",
      "\u001b[92m\u001b[1m[2023-09-24 17:59:35.959] | Success | \u001b[0mInitialized empty <CMU_MOSI_ModifiedTimestampedWords> computational sequence.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:35.959] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:35.961] | Success | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:35.961] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_ModifiedTimestampedWords> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:59:35.961] | Warning | \u001b[0m<CMU_MOSI_ModifiedTimestampedWords> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-24 17:59:35.961] | Success | \u001b[0mInitialized empty <CMU_MOSI_Visual_Facet_41> computational sequence.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:35.961] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_Visual_Facet_41> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:35.963] | Success | \u001b[0m<CMU_MOSI_Visual_Facet_41> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:35.963] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_Visual_Facet_41> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:59:35.963] | Warning | \u001b[0m<CMU_MOSI_Visual_Facet_41> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-24 17:59:35.963] | Success | \u001b[0mInitialized empty <CMU_MOSI_COVAREP> computational sequence.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:35.963] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_COVAREP> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:35.966] | Success | \u001b[0m<CMU_MOSI_COVAREP> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:35.966] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_COVAREP> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:59:35.966] | Warning | \u001b[0m<CMU_MOSI_COVAREP> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2023-09-24 17:59:35.966] | Success | \u001b[0mInitialized empty <CMU_MOSI_Opinion_Labels> computational sequence.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:35.966] | Status  | \u001b[0mChecking the format of the data in <CMU_MOSI_Opinion_Labels> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2023-09-24 17:59:35.969] | Success | \u001b[0m<CMU_MOSI_Opinion_Labels> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2023-09-24 17:59:35.969] | Status  | \u001b[0mChecking the format of the metadata in <CMU_MOSI_Opinion_Labels> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2023-09-24 17:59:35.969] | Warning | \u001b[0m<CMU_MOSI_Opinion_Labels> computational sequence does not have all the required metadata ... continuing \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "label_field = 'CMU_MOSI_Opinion_Labels'\n",
    "label_recipe = {label_field: os.path.join(DATA_PATH, label_field+'.csd')}\n",
    "dataset.add_computational_sequences(label_recipe, destination=None)\n",
    "dataset.align(label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1iG0909rllw[3]\n"
     ]
    }
   ],
   "source": [
    "# check out what the keys look like now\n",
    "print(list(dataset[label_field].keys())[55])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting dataset and normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: 52\n",
      "Shape of validation set: 10\n",
      "Shape of test set: 31\n"
     ]
    }
   ],
   "source": [
    "# Spliting dataset into train, test and evaluation sets\n",
    "DATASET = md.cmu_mosi\n",
    "\n",
    "train_set = DATASET.standard_folds.standard_train_fold\n",
    "valid_set = DATASET.standard_folds.standard_valid_fold\n",
    "test_set = DATASET.standard_folds.standard_test_fold\n",
    "\n",
    "print(f\"Shape of training set: {len(train_set)}\")\n",
    "print(f\"Shape of validation set: {len(valid_set)}\")\n",
    "print(f\"Shape of test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tmZoasNr4rU', 'zhpQhgha_KU', 'lXPQBPVc5Cw', 'iiK8YX8oH1E', 'tStelxIAHjw', 'nzpVDcQ0ywM', 'etzxEpPuc6I', 'cW1FSBF59ik', 'd6hH302o4v8', 'k5Y_838nuGo', 'pLTX3ipuDJI', 'jUzDDGyPkXU', 'f_pcplsH_V0', 'yvsjCA6Y5Fc', 'nbWiPyCm4g0', 'rnaNMUZpvvg', 'wMbj6ajWbic', 'cM3Yna7AavY', 'yDtzw_Y-7RU', 'vyB00TXsimI', 'dq3Nf_lMPnE', 'phBUpBr1hSo', 'd3_k5Xpfmik', 'v0zCBqDeKcE', 'tIrG4oNLFzE', 'fvVhgmXxadc', 'ob23OKe5a9Q', 'cXypl4FnoZo', 'vvZ4IcEtiZc', 'f9O3YtZ2VfI', 'c7UH_rxdZv4']\n"
     ]
    }
   ],
   "source": [
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1k/7l23nvk1643grz07p393jdv40000gn/T/ipykernel_75215/410599388.py:65: RuntimeWarning: invalid value encountered in divide\n",
      "  acoustic = np.nan_to_num((acoustic - acoustic.mean(0, keepdims=True)) / (EPS + np.std(acoustic, axis=0, keepdims=True)))\n",
      "/Users/lernmi/miniconda3/envs/emotionunify/lib/python3.11/site-packages/numpy/core/_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/var/folders/1k/7l23nvk1643grz07p393jdv40000gn/T/ipykernel_75215/410599388.py:64: RuntimeWarning: invalid value encountered in divide\n",
      "  visual = np.nan_to_num((visual - visual.mean(0, keepdims=True)) / (EPS + np.std(visual, axis=0, keepdims=True)))\n",
      "/Users/lernmi/miniconda3/envs/emotionunify/lib/python3.11/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/Users/lernmi/miniconda3/envs/emotionunify/lib/python3.11/site-packages/numpy/core/_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 0 datapoints have been dropped.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "\n",
    "# a sentinel epsilon for safe division, without it we will replace illegal values with a constant\n",
    "EPS = 0\n",
    "\n",
    "# construct a word2id mapping that automatically takes increment when new words are encountered\n",
    "word2id = defaultdict(lambda: len(word2id))\n",
    "UNK = word2id['<unk>']\n",
    "PAD = word2id['<pad>']\n",
    "\n",
    "# place holders for the final train/dev/test dataset\n",
    "train = []\n",
    "validation = []\n",
    "test = []\n",
    "\n",
    "# define a regular expression to extract the video ID out of the keys\n",
    "pattern = re.compile('(.*)\\[.*\\]')\n",
    "num_drop = 0 # a counter to count how many data points went into some processing issues\n",
    "\n",
    "for segment in dataset[label_field].keys():\n",
    "\n",
    "    # get the video ID and the features out of the aligned dataset\n",
    "    vid = re.search(pattern, segment).group(1)\n",
    "    label = dataset[label_field][segment]['features']\n",
    "    _words = dataset[text_field][segment]['features']\n",
    "    _visual = dataset[visual_field][segment]['features']\n",
    "    _acoustic = dataset[acoustic_field][segment]['features']\n",
    "\n",
    "    # if the sequences are not same length after alignment, there must be some problem with some modalities\n",
    "    # we should drop it or inspect the data again\n",
    "    if not _words.shape[0] == _visual.shape[0] == _acoustic.shape[0]:\n",
    "        print(f\"Encountered datapoint {vid} with text shape {_words.shape}, visual shape {_visual.shape}, acoustic shape {_acoustic.shape}\")\n",
    "        num_drop += 1\n",
    "        continue\n",
    "\n",
    "    # remove nan values\n",
    "    label = np.nan_to_num(label)\n",
    "    _visual = np.nan_to_num(_visual)\n",
    "    _acoustic = np.nan_to_num(_acoustic)\n",
    "\n",
    "    # remove speech pause tokens - this is in general helpful\n",
    "    # we should remove speech pauses and corresponding visual/acoustic features together\n",
    "    # otherwise modalities would no longer be aligned\n",
    "    words = []\n",
    "    visual = []\n",
    "    acoustic = []\n",
    "    for i, word in enumerate(_words):\n",
    "        if word[0] != b'sp':\n",
    "            words.append(word2id[word[0].decode('utf-8')]) # SDK stores strings as bytes, decode into strings here\n",
    "            visual.append(_visual[i, :])\n",
    "            acoustic.append(_acoustic[i, :])\n",
    "\n",
    "    words = np.asarray(words)\n",
    "    visual = np.asarray(visual)\n",
    "    acoustic = np.asarray(acoustic)\n",
    "\n",
    "    # z-normalization per instance and remove nan/infs\n",
    "    visual = np.nan_to_num((visual - visual.mean(0, keepdims=True)) / (EPS + np.std(visual, axis=0, keepdims=True)))\n",
    "    acoustic = np.nan_to_num((acoustic - acoustic.mean(0, keepdims=True)) / (EPS + np.std(acoustic, axis=0, keepdims=True)))\n",
    "\n",
    "    if vid in train_set:\n",
    "        train.append(((words, visual, acoustic), label, segment))\n",
    "    elif vid in valid_set:\n",
    "        validation.append(((words, visual, acoustic), label, segment))\n",
    "    elif vid in test_set:\n",
    "        test.append(((words, visual, acoustic), label, segment))\n",
    "    else:\n",
    "        print(f\"Found video that doesn't belong to any splits: {vid}\")\n",
    "\n",
    "print(f\"Total number of {num_drop} datapoints have been dropped.\")\n",
    "\n",
    "# turn off the word2id - define a named function here to allow for pickling\n",
    "def return_unk():\n",
    "    return UNK\n",
    "word2id.default_factory = return_unk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Train set aligned with label added: 1283\n",
      "Test set aligned with label added: 686\n",
      "Validation set aligned with label added: 229\n",
      "================================================================================\n",
      "Shape into sequences: 3\n",
      "Shape into batch: 3\n",
      "================================================================================\n",
      "Text vector shape: (5,)\n",
      "Visual vector shape: (5, 47)\n",
      "Acoustic vector shape: (5, 74)\n"
     ]
    }
   ],
   "source": [
    "print(80*\"=\")\n",
    "print(f'Train set aligned with label added: {len(train)}') \n",
    "print(f'Test set aligned with label added: {len(test)}') \n",
    "print(f'Validation set aligned with label added: {len(validation)}') \n",
    "print(80*\"=\")\n",
    "print(f'Shape into sequences: {len(train[0])}') \n",
    "print(f'Shape into batch: {len(train[0][0])}') \n",
    "print(80*\"=\")\n",
    "print(f'Text vector shape: {train[0][0][0].shape}')\n",
    "print(f'Visual vector shape: {train[0][0][1].shape}')\n",
    "print(f'Acoustic vector shape: {train[0][0][2].shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 4, 5, 6]),\n",
       " array([[-1.99193895e+00, -1.56130433e+00,  2.24037215e-01,\n",
       "          2.24037215e-01, -1.30791950e+00, -5.24096489e-01,\n",
       "          1.08224618e+00,  4.84659851e-01,  6.20539188e-01,\n",
       "         -8.82908583e-01,  1.75792313e+00,  4.26066786e-01,\n",
       "         -8.13734174e-01,  1.10976946e+00,  4.64716285e-01,\n",
       "         -1.12349319e+00, -7.46687233e-01, -5.96176326e-01,\n",
       "          1.65984178e+00,  6.28019050e-02,  8.35817695e-01,\n",
       "         -9.04375374e-01,  1.90320683e+00,  2.73384601e-01,\n",
       "         -7.74063706e-01,  1.00022411e+00,  5.14332838e-02,\n",
       "         -7.44960308e-01,  3.13304812e-01,  6.73187897e-02,\n",
       "          8.88872266e-01, -1.79359889e+00, -1.04357433e+00,\n",
       "          1.11288726e+00, -2.41447568e-01, -5.12846828e-01,\n",
       "         -1.19174254e+00,  6.11311018e-01, -5.77251315e-01,\n",
       "         -8.56405020e-01, -1.29991710e+00,  5.23986220e-01,\n",
       "         -1.32399571e+00, -9.65939343e-01,  1.30997336e+00,\n",
       "          1.44799495e+00,  9.02126491e-01],\n",
       "        [ 5.74711084e-01,  1.06038904e+00, -2.97105342e-01,\n",
       "         -2.97105342e-01,  1.43469846e+00, -8.47869575e-01,\n",
       "         -7.88133442e-01,  7.68965721e-01, -2.82070816e-01,\n",
       "         -7.29235634e-02, -1.21650445e+00,  5.64876080e-01,\n",
       "         -4.72695619e-01, -1.18835950e+00, -1.54848325e+00,\n",
       "         -3.84068757e-01,  1.71588767e+00, -8.66281271e-01,\n",
       "         -7.17658877e-01,  6.28285885e-01,  1.42355397e-01,\n",
       "          1.17085055e-01, -9.44404244e-01,  4.25736904e-01,\n",
       "         -6.66808069e-01, -9.83002901e-01, -8.72651875e-01,\n",
       "         -7.52265394e-01, -5.46098292e-01,  3.66133869e-01,\n",
       "          3.09503168e-01, -2.52752423e-01,  1.38472617e+00,\n",
       "          1.06864309e+00, -1.55483735e+00,  1.44592249e+00,\n",
       "         -1.08731639e+00, -2.11982071e-01, -1.07606828e+00,\n",
       "         -8.47806573e-01, -7.24667907e-01,  1.20478845e+00,\n",
       "         -2.34222710e-01, -3.55985522e-01, -1.53143549e+00,\n",
       "          2.24965155e-01,  1.29731071e+00],\n",
       "        [ 5.91449499e-01, -3.01213980e-01, -1.68665028e+00,\n",
       "         -1.68665028e+00,  2.53856212e-01,  1.28438973e+00,\n",
       "          8.70518148e-01, -1.51642406e+00,  7.92516351e-01,\n",
       "          1.37622643e+00, -1.26406422e-03, -1.46676755e+00,\n",
       "          1.35167146e+00, -3.46050560e-01,  1.31962061e+00,\n",
       "          1.06637049e+00, -4.65919003e-02,  1.35089231e+00,\n",
       "          4.35472488e-01, -1.16669726e+00,  3.07325333e-01,\n",
       "          1.17836583e+00, -1.96485981e-01, -1.29859257e+00,\n",
       "          1.77220607e+00, -5.73528290e-01,  1.82406938e+00,\n",
       "          5.01019597e-01,  1.82440206e-02,  1.19488068e-01,\n",
       "         -1.76551497e+00,  2.80729502e-01, -7.49780834e-01,\n",
       "          9.99387279e-02,  1.07295990e+00, -1.08122754e+00,\n",
       "          9.33049202e-01,  5.09970844e-01,  1.19833827e+00,\n",
       "          1.07668352e+00,  9.76164639e-01, -1.20722818e+00,\n",
       "          1.03736985e+00,  1.27897060e+00, -4.41803992e-01,\n",
       "         -1.06615758e+00, -3.12104821e-02],\n",
       "        [ 4.87453789e-01, -3.14994425e-01,  3.95901620e-01,\n",
       "          3.95901620e-01,  5.49311519e-01,  1.12844193e+00,\n",
       "          3.58854264e-01, -8.27694237e-01, -1.84371817e+00,\n",
       "          8.46398830e-01,  1.06710143e-01, -8.19151521e-01,\n",
       "          1.03631592e+00, -8.23609769e-01,  4.37512785e-01,\n",
       "          1.30590332e+00,  2.70273596e-01,  1.07141006e+00,\n",
       "         -1.45096183e-01, -1.01260066e+00, -1.93898225e+00,\n",
       "          9.67084408e-01, -1.01217359e-01, -8.99844170e-01,\n",
       "          4.51012135e-01, -8.50913167e-01, -5.10103963e-02,\n",
       "          1.74948156e+00, -1.40604508e+00, -1.80850577e+00,\n",
       "         -3.63054663e-01,  1.08480048e+00, -6.16607368e-01,\n",
       "         -1.08184671e+00,  1.10709584e+00, -7.82156408e-01,\n",
       "          1.23024857e+00,  9.43184853e-01,  1.21979570e+00,\n",
       "          1.36058056e+00,  1.32501650e+00, -1.17742980e+00,\n",
       "          1.25551176e+00,  1.09468448e+00, -1.79993480e-01,\n",
       "         -1.17858827e+00, -1.35941005e+00],\n",
       "        [ 3.38322818e-01,  1.11711824e+00,  1.36382139e+00,\n",
       "          1.36382139e+00, -9.29946721e-01, -1.04086602e+00,\n",
       "         -1.52348650e+00,  1.09049392e+00,  7.12733090e-01,\n",
       "         -1.26679277e+00, -6.46865427e-01,  1.29497707e+00,\n",
       "         -1.10155761e+00,  1.24825060e+00, -6.73366368e-01,\n",
       "         -8.64710987e-01, -1.19288290e+00, -9.59844291e-01,\n",
       "         -1.23255908e+00,  1.48821044e+00,  6.53485417e-01,\n",
       "         -1.35816002e+00, -6.61099315e-01,  1.49931538e+00,\n",
       "         -7.82346368e-01,  1.40722024e+00, -9.51841056e-01,\n",
       "         -7.53275514e-01,  1.62059295e+00,  1.25556505e+00,\n",
       "          9.30194676e-01,  6.80821598e-01,  1.02523637e+00,\n",
       "         -1.19962656e+00, -3.83770287e-01,  9.30309176e-01,\n",
       "          1.15761220e-01, -1.85248339e+00, -7.64814436e-01,\n",
       "         -7.33053565e-01, -2.76596516e-01,  6.55883431e-01,\n",
       "         -7.34662771e-01, -1.05173004e+00,  8.43259633e-01,\n",
       "          5.71785629e-01, -8.08816195e-01]], dtype=float32),\n",
       " array([[ 1.2337455 , -1.2865584 , -1.4212074 , -0.98561466, -0.554881  ,\n",
       "         -1.2530013 , -0.08835446,  1.347171  , -0.73087543, -1.0554795 ,\n",
       "          0.28273967, -0.60849464, -1.4422859 , -0.7819848 , -0.5617527 ,\n",
       "         -0.35296094, -0.02896533,  0.60175824,  1.3344476 ,  0.12467237,\n",
       "         -0.11090465, -0.9196475 ,  1.5485014 , -0.6844348 ,  0.42112482,\n",
       "         -0.87435156,  0.95257026, -1.4756348 ,  0.4875557 ,  1.6191478 ,\n",
       "         -0.723377  ,  0.17529337,  0.743496  , -1.2450515 ,  0.4543776 ,\n",
       "          0.34469157,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -0.09082218, -0.5224229 ,\n",
       "         -0.40689698, -0.8052265 , -1.0065228 , -1.0303878 , -1.0251762 ,\n",
       "         -0.7999101 , -0.88762605,  1.3008825 , -1.0669531 , -0.84140706,\n",
       "         -1.6679962 , -0.7544485 ,  1.0617316 , -1.5976877 ,  0.1926626 ,\n",
       "         -1.7908894 ,  1.2296748 ,  1.2308731 ,  1.1788867 ,  0.99051017,\n",
       "          0.8340707 ,  1.7342516 , -0.32687488, -0.7460231 , -1.1373225 ,\n",
       "         -1.2700351 , -1.2877431 , -1.2262467 , -1.2835587 ],\n",
       "        [ 1.074647  , -1.1266305 , -0.54900473, -0.9282314 , -0.3125983 ,\n",
       "         -0.9559189 , -1.1073134 ,  0.34857723,  0.20529433, -0.938075  ,\n",
       "          1.0487635 , -1.1566954 , -0.78036976, -0.58632064, -1.2302734 ,\n",
       "          0.19227737,  0.49068025,  0.89940447,  0.4559686 ,  0.43048722,\n",
       "          1.1807787 , -0.7309116 , -1.0421906 , -0.5099248 ,  1.5949066 ,\n",
       "          0.9319966 , -0.83736545, -0.6112021 ,  1.2187859 , -0.10932878,\n",
       "         -0.5793332 ,  0.3412901 ,  0.49943042, -1.0724478 ,  0.30301863,\n",
       "          0.7039117 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.70756   ,  1.8102696 ,\n",
       "          1.1928737 , -1.4421339 , -1.2948874 , -1.249918  , -1.0221747 ,\n",
       "         -0.94250727, -0.27851093, -0.05589927, -0.11015978, -1.0288332 ,\n",
       "         -0.5433321 , -0.16289367,  0.7212912 , -0.5658691 , -0.40100884,\n",
       "         -0.32844985,  1.1290553 ,  1.1402171 ,  1.2075928 ,  1.1825055 ,\n",
       "          0.736712  , -0.66805756, -0.8078154 , -0.7430187 , -1.121479  ,\n",
       "         -1.1392351 , -1.1497978 , -1.1240605 , -1.0739312 ],\n",
       "        [-1.0991822 ,  1.038759  ,  1.4546305 ,  0.94446707,  1.9204297 ,\n",
       "          1.221448  ,  0.19008975, -0.2576876 ,  0.580528  ,  0.6596706 ,\n",
       "          0.23857516,  0.42382792,  0.36521643,  1.055999  ,  1.4547172 ,\n",
       "          0.10826273,  1.2360445 , -1.4697756 , -0.8465401 ,  0.9947606 ,\n",
       "         -1.7178859 , -0.73598725, -0.08553627,  1.2551278 , -0.69779414,\n",
       "          1.0808734 , -1.4937911 , -0.11862117,  0.5763777 , -0.9403572 ,\n",
       "          0.9209688 ,  0.20174114, -1.7346648 ,  0.2247009 ,  1.2429702 ,\n",
       "         -1.6508875 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -0.09082218,  0.3166831 ,\n",
       "          1.1060466 ,  0.6002335 ,  0.6719273 ,  0.47271457,  0.45714644,\n",
       "          0.22982216,  0.16039036, -1.0962485 ,  1.8934242 ,  0.75132096,\n",
       "          0.9046386 , -1.3202695 , -1.7738312 ,  1.1079339 ,  0.7076948 ,\n",
       "          0.6488784 , -1.1059477 , -0.9870454 , -1.2018663 , -1.2964536 ,\n",
       "         -1.3012007 , -0.3687333 ,  1.9451149 ,  1.9393773 ,  1.3985155 ,\n",
       "          0.5703951 ,  0.79599017,  1.1160502 ,  0.5346432 ],\n",
       "        [-0.16659446,  0.5260907 , -0.20583725, -0.46533224, -0.94599104,\n",
       "         -0.02429766,  1.772498  , -1.7097907 ,  1.4017127 , -0.24989839,\n",
       "         -1.9099085 , -0.38111004,  1.4043307 ,  1.3563255 , -0.5376639 ,\n",
       "          1.5794897 ,  0.09700492,  0.89760435, -1.4335152 , -1.9169896 ,\n",
       "         -0.1409503 ,  1.5327857 ,  0.6322694 , -1.1910594 ,  0.02026974,\n",
       "          0.3112215 ,  0.3654359 ,  1.1404073 , -0.7295129 , -1.097133  ,\n",
       "         -1.0779972 ,  1.1498272 , -0.48482162,  1.2186034 , -1.7604505 ,\n",
       "          1.161053  ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -0.09082218, -0.625556  ,\n",
       "         -0.49976537,  0.2967499 ,  0.29943323,  0.39445567, -0.0507151 ,\n",
       "         -0.31024665, -0.8393381 , -1.0970497 , -0.42088577, -0.4512489 ,\n",
       "          1.0072439 ,  1.3996162 ,  0.31629777,  0.9416343 ,  1.1954153 ,\n",
       "          0.43301922, -0.30601373, -0.33649164, -0.53841114, -0.96534777,\n",
       "         -1.1427892 , -1.1125599 , -0.15151715, -0.34514117,  0.6780252 ,\n",
       "          0.7282936 ,  0.9599467 ,  0.28131363,  0.5565146 ],\n",
       "        [-1.0426174 ,  0.8483393 ,  0.7214187 ,  1.4347118 , -0.10695982,\n",
       "          1.0117713 , -0.7669206 ,  0.2717295 , -1.4566619 ,  1.5837815 ,\n",
       "          0.33983016,  1.7224735 ,  0.45310813, -1.0440187 ,  0.87497246,\n",
       "         -1.5270689 , -1.7947642 , -0.9289915 ,  0.48963907,  0.3670694 ,\n",
       "          0.7889627 ,  0.8537604 , -1.053044  ,  1.1302909 , -1.3385066 ,\n",
       "         -1.4497399 ,  1.0131503 ,  1.0650507 , -1.5532063 ,  0.5276705 ,\n",
       "          1.4597387 , -1.8681519 ,  0.97656   ,  0.8741952 , -0.239916  ,\n",
       "         -0.5587689 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -1.4350934 , -0.97897375,\n",
       "         -1.392258  ,  1.3503774 ,  1.3300498 ,  1.4131354 ,  1.6409198 ,\n",
       "          1.8228418 ,  1.8450847 ,  0.94831496, -0.2954255 ,  1.5701681 ,\n",
       "          0.29944575,  0.8379954 , -0.32548916,  0.11398853, -1.6947637 ,\n",
       "          1.0374426 , -0.9467676 , -1.0475549 , -0.646202  ,  0.08878053,\n",
       "          0.8732073 ,  0.4151104 , -0.6589064 , -0.10519248,  0.18226017,\n",
       "          1.110582  ,  0.6816042 ,  0.95294344,  1.2663324 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Batch shape: 5\n",
      "================================================================================\n",
      "Text shape: torch.Size([78, 8])\n",
      "Visual shape: torch.Size([78, 8, 47])\n",
      "Acoustic shape: torch.Size([78, 8, 74])\n",
      "================================================================================\n",
      "Labels: tensor([[-2.0000],\n",
      "        [ 2.6000],\n",
      "        [ 1.4000],\n",
      "        [-0.4000],\n",
      "        [ 0.6000],\n",
      "        [ 2.2000],\n",
      "        [ 0.8000],\n",
      "        [ 1.2000]])\n",
      "Length: tensor([78, 26, 22, 16, 10,  7,  6,  5])\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def multi_collate(batch):\n",
    "    '''\n",
    "    Collate functions assume batch = [Dataset[i] for i in index_set]\n",
    "    '''\n",
    "    # for later use we sort the batch in descending order of length\n",
    "    batch = sorted(batch, key=lambda x: x[0][0].shape[0], reverse=True)\n",
    "\n",
    "    # get the data out of the batch - use pad sequence util functions from PyTorch to pad things\n",
    "    #print([torch.from_numpy(sample[0]) for sample in batch])\n",
    "    labels = torch.cat([torch.from_numpy(sample[1]) for sample in batch], dim=0)\n",
    "    visual = pad_sequence([torch.FloatTensor(sample[0][1]) for sample in batch])\n",
    "    sentences = pad_sequence([torch.LongTensor(sample[0][0]) for sample in batch], padding_value=PAD)\n",
    "    acoustic = pad_sequence([torch.FloatTensor(sample[0][2]) for sample in batch])\n",
    "\n",
    "    # lengths are useful later in using RNNs\n",
    "    lengths = torch.LongTensor([sample[0][0].shape[0] for sample in batch])\n",
    "    return sentences, visual, acoustic, labels, lengths\n",
    "\n",
    "# construct dataloaders, dev and test could use around ~X3 times batch size since no_grad is used during eval\n",
    "batch_sz = 64\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=batch_sz, collate_fn=multi_collate)\n",
    "validation_loader = DataLoader(validation, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)\n",
    "test_loader = DataLoader(test, shuffle=False, batch_size=batch_sz*3, collate_fn=multi_collate)\n",
    "\n",
    "\n",
    "# let's create a temporary dataloader just to see how the batch looks like\n",
    "temp_loader = iter(DataLoader(test, shuffle=True, batch_size=8, collate_fn=multi_collate))\n",
    "batch = next(temp_loader)\n",
    "\n",
    "\n",
    "print(80*\"=\")\n",
    "print(f\"Batch shape: {len(batch)}\") # word vectors, padded to maxlen\n",
    "print(80*\"=\")\n",
    "print(f\"Text shape: {batch[0].shape}\") # word vectors, padded to maxlen\n",
    "print(f\"Visual shape: {batch[1].shape}\") # visual features\n",
    "print(f\"Acoustic shape: {batch[2].shape}\") # acoustic features\n",
    "print(80*\"=\")\n",
    "print(f\"Labels: {batch[3]}\") # labels\n",
    "print(f\"Length: {batch[4]}\") # lengths\n",
    "print(80*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incredible steve carell voices gru and russell brand actually really impressed me with his old man voice for the\n",
      "[[1.8]]\n",
      "9J25DZhivz8[9]\n"
     ]
    }
   ],
   "source": [
    "id2word = {v:k for k,v in word2id.items()}\n",
    "examine_target = train\n",
    "idx = np.random.randint(0, len(examine_target))\n",
    "print(' '.join(list(map(lambda x: id2word[x], examine_target[idx][0][0].tolist()))))\n",
    "# print(' '.join(examine_target[idx][0]))\n",
    "print(examine_target[idx][1])\n",
    "print(examine_target[idx][2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LateFusionLSTM Multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LateFusionLSTM(nn.Module): \n",
    "    '''\n",
    "    The LateFusionLSTM class is a PyTorch module that implements a late fusion LSTM \n",
    "    model for multimodal data fusion. It takes as input three types of data: text, \n",
    "    visual, and acoustic, and combines them using LSTM layers and concatenation. \n",
    "    The model then applies fully connected layers to produce the final output.\n",
    "\n",
    "    - How to use?\n",
    "        model = LateFusionLSTM(input_sizes=[100, 200, 300], \n",
    "                            hidden_sizes=[50, 50, 50], \n",
    "                            fc1_size=100, output_size=10, \n",
    "                            dropout_rate=0.5)\n",
    "\n",
    "\n",
    "    '''\n",
    "    def __init__(self, input_sizes, hidden_sizes, fc1_size, output_size, dropout_rate):\n",
    "        super(LateFusionLSTM, self).__init__()\n",
    "        self.input_size = input_sizes\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.fc1_size = fc1_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # defining modules - two layer bidirectional LSTM with layer norm in between\n",
    "        self.embed = nn.Embedding(len(word2id), input_sizes[0])\n",
    "        self.trnn1 = nn.LSTM(input_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "        self.trnn2 = nn.LSTM(2*hidden_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "\n",
    "        self.vrnn1 = nn.LSTM(input_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "        self.vrnn2 = nn.LSTM(2*hidden_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "\n",
    "        self.arnn1 = nn.LSTM(input_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "        self.arnn2 = nn.LSTM(2*hidden_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(sum(hidden_sizes)*4, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.tlayer_norm = nn.LayerNorm((hidden_sizes[0]*2,))\n",
    "        self.vlayer_norm = nn.LayerNorm((hidden_sizes[1]*2,))\n",
    "        self.alayer_norm = nn.LayerNorm((hidden_sizes[2]*2,))\n",
    "        self.bn = nn.BatchNorm1d(sum(hidden_sizes)*4)\n",
    "\n",
    "\n",
    "    def extract_features(self, sequence, lengths, rnn1, rnn2, layer_norm):\n",
    "        packed_sequence = pack_padded_sequence(sequence, lengths)\n",
    "        packed_h1, (final_h1, _) = rnn1(packed_sequence)\n",
    "        padded_h1, _ = pad_packed_sequence(packed_h1)\n",
    "        normed_h1 = layer_norm(padded_h1)\n",
    "        packed_normed_h1 = pack_padded_sequence(normed_h1, lengths)\n",
    "        _, (final_h2, _) = rnn2(packed_normed_h1)\n",
    "        return final_h1, final_h2\n",
    "\n",
    "\n",
    "    def fusion(self, sentences, visual, acoustic, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        sentences = self.embed(sentences)\n",
    "\n",
    "        # extract features from text modality\n",
    "        final_h1t, final_h2t = self.extract_features(sentences, lengths, self.trnn1, self.trnn2, self.tlayer_norm)\n",
    "\n",
    "        # extract features from visual modality\n",
    "        final_h1v, final_h2v = self.extract_features(visual, lengths, self.vrnn1, self.vrnn2, self.vlayer_norm)\n",
    "\n",
    "        # extract features from acoustic modality\n",
    "        final_h1a, final_h2a = self.extract_features(acoustic, lengths, self.arnn1, self.arnn2, self.alayer_norm)\n",
    "\n",
    "\n",
    "        # simple late fusion -- concatenation + normalization\n",
    "        h = torch.cat((final_h1t, final_h2t, final_h1v, final_h2v, final_h1a, final_h2a),\n",
    "                       dim=2).permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
    "        return self.bn(h)\n",
    "\n",
    "    def forward(self, sentences, visual, acoustic, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        h = self.fusion(sentences, visual, acoustic, lengths)\n",
    "        h = self.fc1(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(h)\n",
    "        o = self.fc2(h)\n",
    "        return o\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertLateFusionLSTM(nn.Module):\n",
    "    def __init__(self, input_sizes, hidden_sizes, fc1_size, output_size, dropout_rate, bert_model):\n",
    "        super(BertLateFusionLSTM, self).__init__()\n",
    "        self.input_size = input_sizes\n",
    "        self.hidden_size = hidden_sizes\n",
    "        self.fc1_size = fc1_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # defining modules - two layer bidirectional LSTM with layer norm in between\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.trnn1 = nn.LSTM(self.bert.config.hidden_size, hidden_sizes[0], bidirectional=True)\n",
    "        self.trnn2 = nn.LSTM(2*hidden_sizes[0], hidden_sizes[0], bidirectional=True)\n",
    "\n",
    "        self.vrnn1 = nn.LSTM(input_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "        self.vrnn2 = nn.LSTM(2*hidden_sizes[1], hidden_sizes[1], bidirectional=True)\n",
    "\n",
    "        self.arnn1 = nn.LSTM(input_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "        self.arnn2 = nn.LSTM(2*hidden_sizes[2], hidden_sizes[2], bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(sum(hidden_sizes)*4, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.tlayer_norm = nn.LayerNorm((hidden_sizes[0]*2,))\n",
    "        self.vlayer_norm = nn.LayerNorm((hidden_sizes[1]*2,))\n",
    "        self.alayer_norm = nn.LayerNorm((hidden_sizes[2]*2,))\n",
    "        self.bn = nn.BatchNorm1d(sum(hidden_sizes)*4)\n",
    "\n",
    "    def extract_features(self, sequence, lengths, rnn1, rnn2, layer_norm):\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert(sequence)[0]\n",
    "        packed_sequence = pack_padded_sequence(bert_output, lengths)\n",
    "        packed_h1, (final_h1, _) = rnn1(packed_sequence)\n",
    "        padded_h1, _ = pad_packed_sequence(packed_h1)\n",
    "        normed_h1 = layer_norm(padded_h1)\n",
    "        packed_normed_h1 = pack_padded_sequence(normed_h1, lengths)\n",
    "        _, (final_h2, _) = rnn2(packed_normed_h1)\n",
    "        return final_h1, final_h2\n",
    "\n",
    "    def fusion(self, sentences, visual, acoustic, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "\n",
    "        # extract features from text modality\n",
    "        final_h1t, final_h2t = self.extract_features(sentences, lengths, self.trnn1, self.trnn2, self.tlayer_norm)\n",
    "\n",
    "        # extract features from visual modality\n",
    "        final_h1v, final_h2v = self.extract_features(visual, lengths, self.vrnn1, self.vrnn2, self.vlayer_norm)\n",
    "\n",
    "        # extract features from acoustic modality\n",
    "        final_h1a, final_h2a = self.extract_features(acoustic, lengths, self.arnn1, self.arnn2, self.alayer_norm)\n",
    "\n",
    "        # simple late fusion -- concatenation + normalization\n",
    "        h = torch.cat((final_h1t, final_h2t, final_h1v, final_h2v, final_h1a, final_h2a),\n",
    "                       dim=2).permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
    "        return self.bn(h)\n",
    "\n",
    "    def forward(self, sentences, visual, acoustic, lengths):\n",
    "        batch_size = lengths.size(0)\n",
    "        h = self.fusion(sentences, visual, acoustic, lengths)\n",
    "        h = self.fc1(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(h)\n",
    "        o = self.fc2(h)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb(w2i, path_to_embedding, embedding_size=300, embedding_vocab=2196017, init_emb=None):\n",
    "    if init_emb is None:\n",
    "        emb_mat = np.random.randn(len(w2i), embedding_size)\n",
    "    else:\n",
    "        emb_mat = init_emb\n",
    "    f = open(path_to_embedding, 'r')\n",
    "    found = 0\n",
    "    for line in tqdm_notebook(f, total=embedding_vocab):\n",
    "        content = line.strip().split()\n",
    "        vector = np.asarray(list(map(lambda x: float(x), content[-300:])))\n",
    "        word = ' '.join(content[:-300])\n",
    "        if word in w2i:\n",
    "            idx = w2i[word]\n",
    "            emb_mat[idx, :] = vector\n",
    "            found += 1\n",
    "    print(f\"Found {found} words in the embedding file.\")\n",
    "    tensor_emb  = torch.tensor(emb_mat).float()\n",
    "    return tensor_emb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    y_pred_bin = (y_pred.detach().cpu().numpy() >= 0)\n",
    "    y_true_bin = (y_true.detach().cpu().numpy() >= 0)\n",
    "    return accuracy_score(y_true_bin, y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8m5h01cd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bcaf48187c46b2aede5ce71a90cbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-brook-20</strong> at: <a href='https://wandb.ai/ejbejaranos22/Baseline-multimodal/runs/8m5h01cd' target=\"_blank\">https://wandb.ai/ejbejaranos22/Baseline-multimodal/runs/8m5h01cd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230924_200259-8m5h01cd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8m5h01cd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b284f1dadd644d008c2a002a54c0dd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167635188515608, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/lernmi/Desktop/EmotionUnify/src/datasets/CMU-MultimodalSDK/wandb/run-20230924_200937-08jd28by</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ejbejaranos22/Baseline-multimodal/runs/08jd28by' target=\"_blank\">avid-hill-21</a></strong> to <a href='https://wandb.ai/ejbejaranos22/Baseline-multimodal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ejbejaranos22/Baseline-multimodal' target=\"_blank\">https://wandb.ai/ejbejaranos22/Baseline-multimodal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ejbejaranos22/Baseline-multimodal/runs/08jd28by' target=\"_blank\">https://wandb.ai/ejbejaranos22/Baseline-multimodal/runs/08jd28by</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lernmi/miniconda3/envs/emotionunify/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LateFusionLSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, current batch loss: 2.0095:  90%|█████████ | 19/21 [00:11<00:01,  1.89it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from torch.optim import Adam, SGD\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb\n",
    "\n",
    "\n",
    "dropout_values = [0.4, 0.45]\n",
    "# Define the model class to use\n",
    "#name_model = BertLateFusionLSTM\n",
    "name_model = LateFusionLSTM  # This is the class type, not an instance\n",
    "\n",
    "for drop_value in dropout_values:\n",
    "    print(drop_value)\n",
    "    wandb.init(settings=wandb.Settings(start_method=\"fork\"),project=\"Baseline-multimodal\")\n",
    "\n",
    "    # Parameters\n",
    "    torch.manual_seed(123)\n",
    "    torch.cuda.manual_seed_all(123)\n",
    "\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    MAX_EPOCH = 100\n",
    "\n",
    "    text_size = 300\n",
    "    visual_size = 47\n",
    "    acoustic_size = 74\n",
    "\n",
    "    # define some model settings and hyper-parameters\n",
    "    input_sizes = [text_size, visual_size, acoustic_size]\n",
    "    hidden_sizes = [int(text_size * 1.5), int(visual_size * 1.5), int(acoustic_size * 1.5)]\n",
    "    fc1_size = sum(hidden_sizes) // 2\n",
    "    dropout = drop_value\n",
    "    output_size = 1\n",
    "    curr_patience = patience = 10\n",
    "    num_trials = 3\n",
    "    grad_clip_value = 1.3\n",
    "    weight_decay = 0.15\n",
    "\n",
    "    # Configurations\n",
    "    config = wandb.config\n",
    "    config.text_size = text_size\n",
    "    config.visual_size = visual_size\n",
    "    config.acoustic_size = acoustic_size\n",
    "    config.hidden_sizes = hidden_sizes\n",
    "    config.dropout = dropout\n",
    "    config.output_size = output_size\n",
    "    config.patience = patience\n",
    "    config.grad_clip_value = grad_clip_value\n",
    "    config.weight_decay = weight_decay\n",
    "    config.batch_size = batch_sz\n",
    "\n",
    "    if os.path.exists(CACHE_PATH):\n",
    "        pretrained_emb, word2id = torch.load(CACHE_PATH)\n",
    "    elif WORD_EMB_PATH is not None:\n",
    "        pretrained_emb = load_emb(word2id, WORD_EMB_PATH)\n",
    "        torch.save((pretrained_emb, word2id), CACHE_PATH)\n",
    "    else:\n",
    "        pretrained_emb = None\n",
    "\n",
    "    # Create an instance of the model based on the class type\n",
    "    if name_model == LateFusionLSTM:\n",
    "        print(\"Using LateFusionLSTM\")\n",
    "        model = LateFusionLSTM(input_sizes, hidden_sizes, fc1_size, output_size, dropout)\n",
    "        if pretrained_emb is not None:\n",
    "            model.embed.weight.data = pretrained_emb\n",
    "            model.embed.requires_grad = False\n",
    "    elif name_model == BertLateFusionLSTM:\n",
    "        print(\"Using BertLateFusionLSTM\")\n",
    "        model = BertLateFusionLSTM(input_sizes, hidden_sizes, fc1_size, output_size, dropout, bert_model='bert-base-uncased')\n",
    "\n",
    "    optimizer = Adam([param for param in model.parameters() if param.requires_grad], weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "    criterion = nn.L1Loss(reduction='sum')\n",
    "    criterion_test = nn.L1Loss(reduction='sum')\n",
    "    best_valid_loss = float('inf')\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "    lr_scheduler.step() # for some reason it seems the StepLR needs to be stepped once first\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # Training \n",
    "    for e in range(MAX_EPOCH):\n",
    "        # TRAINING MOOD\n",
    "        model.train()\n",
    "        train_iter = tqdm(train_loader)\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0  # Reset valid_total here\n",
    "        for batch in train_iter:\n",
    "            model.zero_grad()\n",
    "            t, v, a, y, l = batch\n",
    "            batch_size = t.size(0)\n",
    "            if CUDA:\n",
    "                t = t.cuda()\n",
    "                v = v.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()\n",
    "                l = l.cuda()\n",
    "            y_tilde = model(t, v, a, l)\n",
    "            loss = criterion(y_tilde, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_([param for param in model.parameters() if param.requires_grad], grad_clip_value)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = calculate_accuracy(y_tilde, y)\n",
    "            train_total += batch_size\n",
    "            train_correct += accuracy * batch_size\n",
    "            train_iter.set_description(f\"Epoch {e}/{MAX_EPOCH}, current batch loss: {round(loss.item()/batch_size, 4)}\")\n",
    "            train_loss += loss.item()\n",
    "            wandb.log({\"Batch Loss\": round(loss.item()/batch_size, 4)})\n",
    "            wandb.log({\"Batch Accuracy\": accuracy})\n",
    "\n",
    "\n",
    "        train_loss = train_loss / len(train)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracy = train_correct / train_total\n",
    "\n",
    "        # Training tracks metrics per epoch\n",
    "        wandb.log({\"Training Loss\": train_loss})\n",
    "        wandb.log({\"Training Accuracy\": train_accuracy})\n",
    "        wandb.log({\"Epoch\": e})\n",
    "        print(f\"[-] Training loss: {round(train_loss, 4)}\")\n",
    "        print(f\"[-] Training accuracy: {train_accuracy}\")\n",
    "\n",
    "        # VALIDATION MOOD\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.0\n",
    "            for batch in validation_loader:\n",
    "                model.zero_grad()\n",
    "                t, v, a, y, l = batch\n",
    "                if CUDA:\n",
    "                    t = t.cuda()\n",
    "                    v = v.cuda()\n",
    "                    a = a.cuda()\n",
    "                    y = y.cuda()\n",
    "                    l = l.cuda()\n",
    "                y_tilde = model(t, v, a, l)\n",
    "                loss = criterion(y_tilde, y)\n",
    "                # calculate accuracy in validation and log in wandb\n",
    "                valid_total += batch_size  # Reset valid_total at the beginning of each epoch\n",
    "                valid_loss += loss.item()\n",
    "                accuracy = calculate_accuracy(y_tilde, y)\n",
    "                valid_correct += accuracy * batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        valid_loss = valid_loss/len(validation)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "        wandb.log({\"Validation Accuracy\": valid_accuracy})\n",
    "        wandb.log({\"Validation Loss\": valid_loss})\n",
    "        print(f\"Validation loss: {round(valid_loss, 4)}\")\n",
    "        print(f\"Validation accuracy: {valid_accuracy}\")\n",
    "        print(f\"Current patience: {curr_patience}, current trial: {num_trials}.\")\n",
    "\n",
    "\n",
    "        if valid_loss <= best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            print(\"Found new best model on dev set!\")\n",
    "            torch.save(model.state_dict(), 'model.std')\n",
    "            torch.save(optimizer.state_dict(), 'optim.std')\n",
    "            curr_patience = patience\n",
    "        else:\n",
    "            curr_patience -= 1\n",
    "            if curr_patience <= -1:\n",
    "                print(\"Running out of patience, loading previous best model.\")\n",
    "                num_trials -= 1\n",
    "                curr_patience = patience\n",
    "                model.load_state_dict(torch.load('model.std'))\n",
    "                optimizer.load_state_dict(torch.load('optim.std'))\n",
    "                lr_scheduler.step()\n",
    "                print(f\"Current learning rate: {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "\n",
    "        if num_trials <= 0:\n",
    "            print(\"Running out of patience, early stopping.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load('model.std'))\n",
    "\n",
    "\n",
    "    # TEST MODE\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for batch in test_loader:\n",
    "            model.zero_grad()\n",
    "            t, v, a, y, l = batch\n",
    "            if CUDA:\n",
    "                t = t.cuda()\n",
    "                v = v.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()\n",
    "                l = l.cuda()\n",
    "            y_tilde = model(t, v, a, l)\n",
    "            loss = criterion_test(y_tilde, y)\n",
    "            y_true.append(y_tilde.detach().cpu().numpy())\n",
    "            y_pred.append(y.detach().cpu().numpy())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f\"Test set performance: {test_loss/len(test)}\")\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    y_true_bin = y_true >= 0\n",
    "    y_pred_bin = y_pred >= 0\n",
    "    bin_acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "    wandb.log({\"Test Loss\": test_loss/len(test), \"Test Accuracy\": bin_acc})\n",
    "    print(f\"Test set accuracy is {bin_acc}\")\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.45\n",
      "0.5\n",
      "0.55\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "dropout_values = [0.4, 0.45, 0.5, 0.55, 0.6]\n",
    "for drop_value in dropout_values:\n",
    "    print(drop_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotionunify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
