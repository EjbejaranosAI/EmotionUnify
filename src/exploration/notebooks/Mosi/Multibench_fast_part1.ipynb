{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+aunLXgHQLmcJH0LS/8Yv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EjbejaranosAI/EmotionUnify/blob/main/Multibench_fast_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0p6_OHxV4eQ",
        "outputId": "b2f21094-f485-4bfd-9471-0773678524e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MultiBench'...\n",
            "remote: Enumerating objects: 6931, done.\u001b[K\n",
            "remote: Counting objects: 100% (142/142), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 6931 (delta 65), reused 121 (delta 60), pack-reused 6789\u001b[K\n",
            "Receiving objects: 100% (6931/6931), 51.06 MiB | 19.25 MiB/s, done.\n",
            "Resolving deltas: 100% (4251/4251), done.\n",
            "/content/MultiBench\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pliang279/MultiBench.git\n",
        "%cd MultiBench"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import sys\n",
        "import gdown\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Download the file to the directory\n",
        "url = 'https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU'\n",
        "output = 'data/mosi_raw.pkl'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "3APMwSwTV6Pr",
        "outputId": "007c25f9-179c-4f96-e0a1-5a31ffa9e1f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU\n",
            "To: /content/MultiBench/data/mosi_raw.pkl\n",
            "100%|██████████| 357M/357M [00:02<00:00, 130MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/mosi_raw.pkl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import the associated dataloader for affect datasets, which MOSI is a part of.\n",
        "from datasets.affect.get_data import get_dataloader\n",
        "\n",
        "# Create the training, validation, and test-set dataloaders.\n",
        "traindata, validdata, testdata = get_dataloader(\n",
        "    '/content/MultiBench/data/mosi_raw.pkl', robust_test=False, max_pad=True, data_type='mosi', max_seq_len=50)"
      ],
      "metadata": {
        "id": "UwuxT12wV96i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Here, we'll import several common modules should you want to mess with this more.\n",
        "from unimodals.common_models import GRU, MLP, Sequential, Identity\n",
        "\n",
        "# As this example is meant to be simple and easy to train, we'll pass in identity\n",
        "# functions for each of the modalities in MOSI:\n",
        "encoders = [Identity().cuda(), Identity().cuda(), Identity().cuda()]"
      ],
      "metadata": {
        "id": "s9HJF_AuV_xC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import a fusion paradigm, in this case early concatenation.\n",
        "from fusions.common_fusions import ConcatEarly  # noqa\n",
        "\n",
        "# Initialize the fusion module\n",
        "fusion = ConcatEarly().cuda()\n",
        ""
      ],
      "metadata": {
        "id": "_CZncKfEWCdJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw9NyaiVXqC4",
        "outputId": "73fe4216-7d13-4ba0-d2c9-9593edd718e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x509AJHcXs9U",
        "outputId": "e1e7b602-2708-4976-a4b8-24a74ef7dcdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head = Sequential(GRU(409, 512, dropout=True, has_padding=False,\n",
        "                  batch_first=True, last_only=True), MLP(512, 512, 1)).cuda()"
      ],
      "metadata": {
        "id": "F8DSIcC8XHEl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x44D4JLFYMtM",
        "outputId": "bb33e4b4-ac56-4d3a-88a3-f676f02fa3eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\n",
            "Installing collected packages: memory_profiler\n",
            "Successfully installed memory_profiler-0.61.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Standard supervised learning training loop\n",
        "from training_structures.Supervised_Learning import train, test\n",
        "\n",
        "# For more information regarding parameters for any system, feel free to check out the documentation\n",
        "# at multibench.readthedocs.io!\n",
        "train(encoders, fusion, head, traindata, validdata, 100, task=\"regression\", optimtype=torch.optim.AdamW,\n",
        "      is_packed=False, lr=1e-3, save='mosi_ef_r0.pt', weight_decay=0.01, objective=torch.nn.L1Loss())\n",
        "\n",
        "print(\"Testing:\")\n",
        "model = torch.load('mosi_ef_r0.pt').cuda()\n",
        "test(model, testdata, 'affect', is_packed=False,\n",
        "     criterion=torch.nn.L1Loss(), task=\"posneg-classification\", no_robust=True)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQNNDhatXKUZ",
        "outputId": "6d1c0c49-835d-4df5-da2d-17d51126d225"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 train loss: tensor(1.3353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 0 valid loss: 1.3834203481674194\n",
            "Saving Best\n",
            "Epoch 1 train loss: tensor(1.3278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 1 valid loss: 1.3926174640655518\n",
            "Epoch 2 train loss: tensor(1.3206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 2 valid loss: 1.391062617301941\n",
            "Epoch 3 train loss: tensor(1.3234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 3 valid loss: 1.3722001314163208\n",
            "Saving Best\n",
            "Epoch 4 train loss: tensor(1.3202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 4 valid loss: 1.3868460655212402\n",
            "Epoch 5 train loss: tensor(1.3177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 5 valid loss: 1.3763071298599243\n",
            "Epoch 6 train loss: tensor(1.3176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 6 valid loss: 1.378089189529419\n",
            "Epoch 7 train loss: tensor(1.3165, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 7 valid loss: 1.3772833347320557\n",
            "Epoch 8 train loss: tensor(1.3186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 8 valid loss: 1.3819822072982788\n",
            "Epoch 9 train loss: tensor(1.3189, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 9 valid loss: 1.3914583921432495\n",
            "Epoch 10 train loss: tensor(1.3195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 10 valid loss: 1.3781496286392212\n",
            "Epoch 11 train loss: tensor(1.3172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 11 valid loss: 1.383561134338379\n",
            "Epoch 12 train loss: tensor(1.3129, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 12 valid loss: 1.4102901220321655\n",
            "Epoch 13 train loss: tensor(1.3042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 13 valid loss: 1.3695732355117798\n",
            "Saving Best\n",
            "Epoch 14 train loss: tensor(1.3214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 14 valid loss: 1.3818427324295044\n",
            "Epoch 15 train loss: tensor(1.3168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 15 valid loss: 1.3849196434020996\n",
            "Epoch 16 train loss: tensor(1.3160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 16 valid loss: 1.3853081464767456\n",
            "Epoch 17 train loss: tensor(1.3138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 17 valid loss: 1.3951910734176636\n",
            "Epoch 18 train loss: tensor(1.3070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 18 valid loss: 1.385968565940857\n",
            "Epoch 19 train loss: tensor(1.2944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 19 valid loss: 1.4729384183883667\n",
            "Epoch 20 train loss: tensor(1.2939, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 20 valid loss: 1.369195580482483\n",
            "Saving Best\n",
            "Epoch 21 train loss: tensor(1.2444, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 21 valid loss: 1.4105857610702515\n",
            "Epoch 22 train loss: tensor(1.1983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 22 valid loss: 1.3652420043945312\n",
            "Saving Best\n",
            "Epoch 23 train loss: tensor(1.1645, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 23 valid loss: 1.385087490081787\n",
            "Epoch 24 train loss: tensor(1.1370, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 24 valid loss: 1.490917444229126\n",
            "Epoch 25 train loss: tensor(1.1320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 25 valid loss: 1.7242684364318848\n",
            "Epoch 26 train loss: tensor(1.1387, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 26 valid loss: 1.4435687065124512\n",
            "Epoch 27 train loss: tensor(1.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 27 valid loss: 1.3774524927139282\n",
            "Epoch 28 train loss: tensor(1.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 28 valid loss: 1.4097685813903809\n",
            "Epoch 29 train loss: tensor(1.0286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 29 valid loss: 1.3440322875976562\n",
            "Saving Best\n",
            "Epoch 30 train loss: tensor(1.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 30 valid loss: 1.3714028596878052\n",
            "Epoch 31 train loss: tensor(0.9800, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 31 valid loss: 1.3103429079055786\n",
            "Saving Best\n",
            "Epoch 32 train loss: tensor(0.9526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 32 valid loss: 1.6322394609451294\n",
            "Epoch 33 train loss: tensor(0.9677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 33 valid loss: 1.3221518993377686\n",
            "Epoch 34 train loss: tensor(0.9004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 34 valid loss: 1.4090430736541748\n",
            "Epoch 35 train loss: tensor(0.9121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 35 valid loss: 1.308855414390564\n",
            "Saving Best\n",
            "Epoch 36 train loss: tensor(0.8502, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 36 valid loss: 1.2741039991378784\n",
            "Saving Best\n",
            "Epoch 37 train loss: tensor(0.8228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 37 valid loss: 1.444806456565857\n",
            "Epoch 38 train loss: tensor(0.8076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 38 valid loss: 1.33352530002594\n",
            "Epoch 39 train loss: tensor(0.7937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 39 valid loss: 1.2590477466583252\n",
            "Saving Best\n",
            "Epoch 40 train loss: tensor(0.7606, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 40 valid loss: 1.4462870359420776\n",
            "Epoch 41 train loss: tensor(0.7273, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 41 valid loss: 1.3243902921676636\n",
            "Epoch 42 train loss: tensor(0.7151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 42 valid loss: 1.2236393690109253\n",
            "Saving Best\n",
            "Epoch 43 train loss: tensor(0.7073, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 43 valid loss: 1.2671279907226562\n",
            "Epoch 44 train loss: tensor(0.6768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 44 valid loss: 1.312150239944458\n",
            "Epoch 45 train loss: tensor(0.6705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 45 valid loss: 1.277909278869629\n",
            "Epoch 46 train loss: tensor(0.6449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 46 valid loss: 1.2856954336166382\n",
            "Epoch 47 train loss: tensor(0.6150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 47 valid loss: 1.2528407573699951\n",
            "Epoch 48 train loss: tensor(0.5949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 48 valid loss: 1.248252272605896\n",
            "Epoch 49 train loss: tensor(0.6059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 49 valid loss: 1.3185824155807495\n",
            "Epoch 50 train loss: tensor(0.5890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 50 valid loss: 1.2455360889434814\n",
            "Epoch 51 train loss: tensor(0.5705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 51 valid loss: 1.300680160522461\n",
            "Epoch 52 train loss: tensor(0.5638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 52 valid loss: 1.3493081331253052\n",
            "Epoch 53 train loss: tensor(0.5571, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 53 valid loss: 1.2698040008544922\n",
            "Epoch 54 train loss: tensor(0.5375, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 54 valid loss: 1.336368441581726\n",
            "Epoch 55 train loss: tensor(0.5087, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 55 valid loss: 1.3030205965042114\n",
            "Epoch 56 train loss: tensor(0.4856, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 56 valid loss: 1.2662376165390015\n",
            "Epoch 57 train loss: tensor(0.4816, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 57 valid loss: 1.3516209125518799\n",
            "Epoch 58 train loss: tensor(0.4890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 58 valid loss: 1.27748441696167\n",
            "Epoch 59 train loss: tensor(0.4827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 59 valid loss: 1.4222652912139893\n",
            "Epoch 60 train loss: tensor(0.4922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 60 valid loss: 1.3527618646621704\n",
            "Epoch 61 train loss: tensor(0.4739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 61 valid loss: 1.2367846965789795\n",
            "Epoch 62 train loss: tensor(0.4959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 62 valid loss: 1.2864503860473633\n",
            "Epoch 63 train loss: tensor(0.4408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 63 valid loss: 1.296568751335144\n",
            "Epoch 64 train loss: tensor(0.4425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 64 valid loss: 1.3193058967590332\n",
            "Epoch 65 train loss: tensor(0.4210, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 65 valid loss: 1.2545711994171143\n",
            "Epoch 66 train loss: tensor(0.4250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 66 valid loss: 1.2860928773880005\n",
            "Epoch 67 train loss: tensor(0.4205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 67 valid loss: 1.2558574676513672\n",
            "Epoch 68 train loss: tensor(0.3852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 68 valid loss: 1.283215880393982\n",
            "Epoch 69 train loss: tensor(0.3879, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 69 valid loss: 1.3665883541107178\n",
            "Epoch 70 train loss: tensor(0.4534, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 70 valid loss: 1.3253726959228516\n",
            "Epoch 71 train loss: tensor(0.3824, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 71 valid loss: 1.282227635383606\n",
            "Epoch 72 train loss: tensor(0.4169, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 72 valid loss: 1.2999595403671265\n",
            "Epoch 73 train loss: tensor(0.3631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 73 valid loss: 1.2626821994781494\n",
            "Epoch 74 train loss: tensor(0.3762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 74 valid loss: 1.3287333250045776\n",
            "Epoch 75 train loss: tensor(0.3826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 75 valid loss: 1.2825820446014404\n",
            "Epoch 76 train loss: tensor(0.3590, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 76 valid loss: 1.2569986581802368\n",
            "Epoch 77 train loss: tensor(0.3657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 77 valid loss: 1.2116788625717163\n",
            "Saving Best\n",
            "Epoch 78 train loss: tensor(0.3452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 78 valid loss: 1.2115050554275513\n",
            "Saving Best\n",
            "Epoch 79 train loss: tensor(0.3480, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 79 valid loss: 1.2686870098114014\n",
            "Epoch 80 train loss: tensor(0.3286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 80 valid loss: 1.2567367553710938\n",
            "Epoch 81 train loss: tensor(0.3275, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 81 valid loss: 1.241377830505371\n",
            "Epoch 82 train loss: tensor(0.3173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 82 valid loss: 1.2646384239196777\n",
            "Epoch 83 train loss: tensor(0.3366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 83 valid loss: 1.2261700630187988\n",
            "Epoch 84 train loss: tensor(0.3121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 84 valid loss: 1.2755653858184814\n",
            "Epoch 85 train loss: tensor(0.3047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 85 valid loss: 1.2322783470153809\n",
            "Epoch 86 train loss: tensor(0.3104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 86 valid loss: 1.245463490486145\n",
            "Epoch 87 train loss: tensor(0.2998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 87 valid loss: 1.265419840812683\n",
            "Epoch 88 train loss: tensor(0.3087, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 88 valid loss: 1.2525883913040161\n",
            "Epoch 89 train loss: tensor(0.3077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 89 valid loss: 1.246627926826477\n",
            "Epoch 90 train loss: tensor(0.2860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 90 valid loss: 1.2803765535354614\n",
            "Epoch 91 train loss: tensor(0.2998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 91 valid loss: 1.2593109607696533\n",
            "Epoch 92 train loss: tensor(0.2790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 92 valid loss: 1.2446662187576294\n",
            "Epoch 93 train loss: tensor(0.3058, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 93 valid loss: 1.225829839706421\n",
            "Epoch 94 train loss: tensor(0.2768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 94 valid loss: 1.268013596534729\n",
            "Epoch 95 train loss: tensor(0.2859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 95 valid loss: 1.2673649787902832\n",
            "Epoch 96 train loss: tensor(0.2645, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 96 valid loss: 1.2091929912567139\n",
            "Saving Best\n",
            "Epoch 97 train loss: tensor(0.2709, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 97 valid loss: 1.2316724061965942\n",
            "Epoch 98 train loss: tensor(0.2802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 98 valid loss: 1.2483071088790894\n",
            "Epoch 99 train loss: tensor(0.2592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch 99 valid loss: 1.2367899417877197\n",
            "Training Time: 108.11814451217651\n",
            "Training Peak Mem: 1225.0859375\n",
            "Training Params: 1680897\n",
            "Testing:\n",
            "acc: 0.649390243902439, 0.6472303206997084\n",
            "Inference Time: 0.5146119594573975\n",
            "Inference Params: 1680897\n"
          ]
        }
      ]
    }
  ]
}